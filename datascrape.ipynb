{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import concurrent.futures\n",
    "from fake_useragent import UserAgent\n",
    "import cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YellowPages main page URL\n",
    "soloPageUrl = 'https://www.yellow-pages.ph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional \n",
    "def add_spaces_after_punctuation(text):\n",
    "    return re.sub(r'([,.;:!?)])([^\\s])', r'\\1 \\2', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_business(business):\n",
    "\n",
    "    # stores the reviews in this list per batch\n",
    "    reviews = []\n",
    "\n",
    "    # Access Page\n",
    "    about = business.div.h2.a['href']\n",
    "\n",
    "    # Page source\n",
    "    pageUrl = soloPageUrl + about\n",
    "    \n",
    "    # adds headers to the request to avoid being blocked\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.google.com/',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    scraper = cloudscraper.create_scraper()  # create a Cloudscraper instance\n",
    "    # use the scraper to get the page source\n",
    "    aboutUrl = scraper.get(pageUrl, headers=headers).text\n",
    "    soupPage = BeautifulSoup(aboutUrl, 'lxml')\n",
    "    singlePage = soupPage.find_all('div', class_='yp-container')\n",
    "\n",
    "    # fetches the reviews in every businness page. every instance is a new page\n",
    "    for page in singlePage:\n",
    "        time.sleep(random.uniform(1, 5))  # add random delay between requests\n",
    "        review_text = page.find('div', class_='yp-see-morex text-break')\n",
    "        if review_text:\n",
    "            reviews.append(review_text.text.replace('\\n', ''))\n",
    "        else:\n",
    "            reviews.append('')\n",
    "\n",
    "    # fetches the necessary information in the main search page\n",
    "    time.sleep(random.uniform(3, 5))\n",
    "    # Search for name\n",
    "    name = business.find(\n",
    "        'h2', class_='search-tradename').text.replace('\\n', '')\n",
    "\n",
    "    # Search for address\n",
    "    address = business.find('span', class_='ellipsis').text\n",
    "\n",
    "    # Search for mobile number\n",
    "    mobile = business.find(\n",
    "        'a', class_='btn btn-yp-default mr-2 biz-btn-call yp-click')\n",
    "    mobileNum = mobile['data-phone'] if mobile else ''\n",
    "\n",
    "    # Search for average rating\n",
    "    star_average = page.find('div', class_='rating-num')\n",
    "    if star_average:\n",
    "        star_average = star_average.text.replace('\\n', '')\n",
    "    else:\n",
    "        star_average = ''\n",
    "\n",
    "    if reviews:\n",
    "        writer.writerow([name, address, mobileNum,\n",
    "                        star_average, reviews.pop(0)])\n",
    "    else:\n",
    "        writer.writerow([name, address, mobileNum, star_average, ''])\n",
    "\n",
    "# steps to scrape. every step is a whole batch of result page\n",
    "steps = 90\n",
    "\n",
    "# user input for starting value. this is to continue the scraping in case of being blocked\n",
    "start_page = input(\"Enter start value: \")\n",
    "start_page = int(start_page)\n",
    "\n",
    "# add headers to the request to avoid being blocked\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# opens broker.csv file to append new scraped data\n",
    "with open(\"broker.csv\", \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # starts automated scraping with error handling. every instance (i) will be added with + 1 to simulate the URL\n",
    "    # of the next page. i is concatenated to the URL to scrape the next page.\n",
    "    for i in range(start_page, steps+1):\n",
    "        htmlText = f'https://www.yellow-pages.ph/category/residential-properties/page-{i}'\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        try:\n",
    "            getHtml = scraper.get(htmlText, headers=headers).text\n",
    "            soup = BeautifulSoup(getHtml, 'lxml')\n",
    "            scraped = soup.find_all('div', class_='search-listing')\n",
    "            # raises this exception if no data was scraped. it means that the IP was blocked and the webpage won't load.\n",
    "            if not scraped:\n",
    "                raise Exception(\n",
    "                    \"Refresh IP, change start value, and try again.\")\n",
    "        except Exception as e:\n",
    "            clear_output()\n",
    "            # prints the blocked IP warning and the final step\n",
    "            print(f\"Blocked IP and the final step was {i}. {e}\")\n",
    "            break\n",
    "        \n",
    "        # prints the URL and the current step being scraped to check if the scraping is still running\n",
    "        print(\" ========================================================================\")\n",
    "        print(\" URL: \", htmlText)\n",
    "        print(f\" Step {i} scraping...\")\n",
    "\n",
    "        # get list of businesses for this page\n",
    "        businesses = soup.find_all('div', class_='search-listing')\n",
    "\n",
    "        # scrape each business in parallel. \n",
    "        # this will speed up the scraping process and make it more efficient\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(scrape_business, business)\n",
    "                       for business in businesses]\n",
    "            # add random delay between requests\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "\n",
    "        time.sleep(5)\n",
    "        # prints the finished batch and the name of the businesses scraped\n",
    "        print(f\" Step {i} finished successfully.\")\n",
    "        print(\" Displaying results...\")\n",
    "        print(\" ========================================================================\")\n",
    "        print(\" Page\", i, \"& step\", steps)\n",
    "        print(\" Names of businesses: \")\n",
    "        for business in businesses:\n",
    "            name = business.find(\n",
    "                'h2', class_='search-tradename').text.replace('\\n', '')\n",
    "            print(\" \", name)\n",
    "        print(\" ========================================================================\")\n",
    "        time.sleep(random.uniform(5, 8))\n",
    "        clear_output()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac2b69d10c2320dd5c4f4d37e3812eaabdef1dc96bb5ad98affb69b86075552"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
